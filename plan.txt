Below er en komplett **"fra 0 til MVP pÃ¥ Ã©n kveld"-plan** for StandardGPT, basert pÃ¥ det som faktisk er pÃ¥ plass. Alle filer, environment-variabler og prompts er allerede klare - koden kan kjÃ¸res direkte.

---

## 0. Status - alt er pÃ¥ plass âœ…

| âœ… Ferdig                                                                | Hvor det finnes                              |
| -------------------------------------------------------------------------- | -------------------------------------------- |
| **Prompts** (analysis / rewriteWithNumb / rewriteWithOut / validate / answer) | `src/prompts/*.txt`                          |
| **Custom embedding API** endpoint                                        | `https://fastembed-api.onrender.com/embed`  |
| **Elasticsearch** endpoint og API-nÃ¸kkel                                 | `.env` fil                                   |
| **OpenAI API** nÃ¸kkel                                                    | `.env` fil                                   |

Kan kjÃ¸res direkte med `python -m src.cli ask "Ditt spÃ¸rsmÃ¥l"`

---

## 1. Faktisk filstruktur (som matcher det som er pÃ¥ plass)

```
standardgpt/
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env                    # âœ… Ferdig fylt ut  
â”œâ”€â”€ .env.example           
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ src/
    â”œâ”€â”€ config.py
    â”œâ”€â”€ prompts/
    â”‚   â”œâ”€â”€ analysis.txt        # âœ… Ferdig
    â”‚   â”œâ”€â”€ rewriteWithNumb.txt # âœ… Ferdig (nÃ¥r standardnummer detekteres)
    â”‚   â”œâ”€â”€ rewriteWithOut.txt  # âœ… Ferdig (uten standardnummer)  
    â”‚   â”œâ”€â”€ validate.txt        # âœ… Ferdig
    â”‚   â””â”€â”€ answer.txt          # âœ… Ferdig
    â”œâ”€â”€ custom_embeddings.py   # Ny fil - custom API-kall
    â”œâ”€â”€ retriever.py
    â”œâ”€â”€ chain.py               # Oppdatert med branching-logikk
    â””â”€â”€ cli.py
```

### .env (allerede fylt ut)

```
EMBEDDING_API_ENDPOINT=https://fastembed-api.onrender.com/embed
ELASTICSEARCH_API_ENDPOINT=https://my-elasticsearch-project-f89a7b.es.eastus.azure.elastic.cloud:443/standard_prod/_search
ELASTICSEARCH_API_KEY=ApiKey UktaQ0I1Y0JuRWlsdGhiTlFRNG06ZXhSZkczenlydk5tOHk1WklYUUFNQQ==
OPENAI_API_KEY=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
OPENAI_MODEL=gpt-4o
```

---

## 2. Installer pakker

```bash
pip install -r requirements.txt
```

**requirements.txt**

```
langchain==0.3.25
langchain-openai>=0.1.1
langchain-elasticsearch>=0.2.0
python-dotenv>=1.0.1
typer>=0.12
requests>=2.31.0
```

---

## 3. `src/config.py`

```python
import os
from dotenv import load_dotenv

load_dotenv()

# Elasticsearch
ELASTICSEARCH_API_ENDPOINT = os.getenv("ELASTICSEARCH_API_ENDPOINT")
ELASTICSEARCH_API_KEY = os.getenv("ELASTICSEARCH_API_KEY")

# Custom Embedding API  
EMBEDDING_API_ENDPOINT = os.getenv("EMBEDDING_API_ENDPOINT")

# OpenAI
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
```

---

## 4. `src/custom_embeddings.py` - Custom embedding API

```python
import requests
from langchain_core.embeddings import Embeddings
from typing import List
from .config import EMBEDDING_API_ENDPOINT

class FastEmbedAPI(Embeddings):
    def __init__(self, api_url: str = None):
        self.api_url = api_url or EMBEDDING_API_ENDPOINT

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        """Embed a list of documents."""
        embeddings = []
        for text in texts:
            response = requests.post(
                self.api_url,
                json={"text": text},
                headers={"Content-Type": "application/json"}
            )
            response.raise_for_status()
            embedding_data = response.json()
            
            # HÃ¥ndter forskjellige response-formater
            if "embedding" in embedding_data:
                embeddings.append(embedding_data["embedding"])
            elif isinstance(embedding_data, list):
                embeddings.append(embedding_data)
            else:
                # Fallback - ta fÃ¸rste verdi
                embeddings.append(list(embedding_data.values())[0])
        return embeddings

    def embed_query(self, text: str) -> List[float]:
        """Embed a query."""
        response = requests.post(
            self.api_url,
            json={"text": text},
            headers={"Content-Type": "application/json"}
        )
        response.raise_for_status()
        embedding_data = response.json()
        
        if "embedding" in embedding_data:
            return embedding_data["embedding"]
        elif isinstance(embedding_data, list):
            return embedding_data
        else:
            return list(embedding_data.values())[0]
```

---

## 5. `src/retriever.py`

```python
from langchain_elasticsearch import ElasticsearchStore
from .custom_embeddings import FastEmbedAPI
from .config import ELASTICSEARCH_API_ENDPOINT, ELASTICSEARCH_API_KEY
import re

# Parse URL for separate host and index
def parse_elasticsearch_url(url):
    # URL format: https://host:port/index/_search
    match = re.match(r'(https?://[^/]+)/([^/]+)/_search', url)
    if match:
        return match.group(1), match.group(2)
    else:
        # Fallback hvis format er annerledes
        return url.rsplit('/', 2)[0], "standard_prod"

es_host, es_index = parse_elasticsearch_url(ELASTICSEARCH_API_ENDPOINT)

embeddings = FastEmbedAPI()

store = ElasticsearchStore(
    es_url=es_host,
    es_api_key=ELASTICSEARCH_API_KEY,
    index_name=es_index,
    embedding=embeddings,
    vector_field="vector",
)

retriever = store.as_retriever(
    search_kwargs={
        "k": 6,
    }
)
```

---

## 6. `src/chain.py` - Chain med branching-logikk

```python
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from .retriever import retriever
from .config import OPENAI_MODEL
from pathlib import Path

def load_prompt(name):
    path = Path(__file__).parent / "prompts" / f"{name}.txt"
    with open(path, 'r', encoding='utf-8') as f:
        content = f.read()
    return PromptTemplate.from_template(content)

# Load alle prompts (med faktiske filnavn)
analysis_prompt = load_prompt("analysis")
rewrite_with_number_prompt = load_prompt("rewriteWithNumb")
rewrite_without_number_prompt = load_prompt("rewriteWithOut")  
validate_prompt = load_prompt("validate")
answer_prompt = load_prompt("answer")

llm = ChatOpenAI(model=OPENAI_MODEL, temperature=0)

def parse_analysis_result(analysis_output):
    """Parse analysis output - returnerer true/false som string"""
    result = analysis_output.content.strip().lower()
    return "true" in result

def route_rewrite(input_data):
    """Route til riktig rewrite basert pÃ¥ analysis"""
    if isinstance(input_data, dict):
        analysis_result = input_data.get("analysis_result")
        question = input_data.get("question")
    else:
        # Direkte fra forrige steg
        analysis_result = input_data
        question = None
        
    needs_number = parse_analysis_result(analysis_result)
    
    if needs_number:
        return rewrite_with_number_prompt
    else:
        return rewrite_without_number_prompt

def retrieve_documents(rewrite_output):
    """Hent dokumenter basert pÃ¥ optimized question"""
    optimized_question = rewrite_output.content.strip()
    docs = retriever.invoke(optimized_question)
    
    return {
        "optimized_question": optimized_question,
        "docs": docs,
        "chunks": "\n\n".join([doc.page_content for doc in docs])
    }

def prepare_validate_input(retrieval_output):
    """Prepare input for validation step"""
    return {
        "question": retrieval_output.get("question", ""),
        "chunks": retrieval_output.get("chunks", ""),
        "answer": retrieval_output.get("answer", "")
    }

def prepare_answer_input(retrieval_output):
    """Prepare input for final answer"""
    return {
        "last_utterance": retrieval_output.get("question", ""),
        "chunks": retrieval_output.get("chunks", "")
    }

# Main chain
def create_chain():
    def process_question(input_data):
        question = input_data.get("question") or input_data
        
        # Step 1: Analysis
        analysis_result = analysis_prompt.invoke({"last_utterance": question})
        analysis_output = llm.invoke(analysis_result)
        
        # Step 2: Route til riktig rewrite
        rewrite_prompt = route_rewrite(analysis_output)
        rewrite_input = rewrite_prompt.invoke({"last_utterance": question})
        rewrite_output = llm.invoke(rewrite_input)
        
        # Step 3: Retrieve documents
        retrieval_result = retrieve_documents(rewrite_output)
        retrieval_result["question"] = question
        
        # Step 4: Generate answer
        answer_input = prepare_answer_input(retrieval_result)
        answer_prompt_filled = answer_prompt.invoke(answer_input)
        final_answer = llm.invoke(answer_prompt_filled)
        
        return {
            "question": question,
            "analysis": analysis_output.content,
            "rewrite": rewrite_output.content,
            "retrieved_docs": len(retrieval_result["docs"]),
            "answer": final_answer.content
        }
    
    return RunnableLambda(process_question)

chain = create_chain()
```

---

## 7. `src/cli.py` - Test CLI

```python
import typer
import json
from .chain import chain

app = typer.Typer(help="StandardGPT CLI")

@app.command()
def ask(question: str):
    """Still et spÃ¸rsmÃ¥l til StandardGPT"""
    try:
        response = chain.invoke({"question": question})
        
        print(f"\nğŸ¤– StandardGPT Svar:")
        print(f"ğŸ“ SpÃ¸rsmÃ¥l: {response['question']}")
        print(f"ğŸ” Analyse: {response['analysis']}")
        print(f"âœï¸  Rewrite: {response['rewrite']}")
        print(f"ğŸ“š Hentet dokumenter: {response['retrieved_docs']}")
        print(f"\nğŸ’¬ Svar:\n{response['answer']}")
        
    except Exception as e:
        typer.echo(f"âŒ Feil: {e}", err=True)

if __name__ == "__main__":
    app()
```

---

## 8. Test kjÃ¸ring

```bash
# Test grunnleggende funksjonalitet
python -m src.cli ask "Hva er kravene til ledningsmerker i bolig?"

# Test med standardnummer
python -m src.cli ask "Kan du forklare NS 3457-7?"

# Test uten standardnummer  
python -m src.cli ask "Hvordan fungerer en varmepumpe?"
```

---

## 9. Logging & debug

Legg til Ã¸verst i `src/chain.py` for detaljert logging:

```python
import logging
logging.basicConfig(level=logging.INFO)
```

---

## 10. API deployment (valgfritt)

```bash
# Installer FastAPI hvis du vil deploye som API
pip install fastapi uvicorn

# Lag server.py
cat > server.py << 'EOF'
from fastapi import FastAPI
from pydantic import BaseModel
from src.chain import chain

app = FastAPI(title="StandardGPT API")

class Question(BaseModel):
    question: str

@app.post("/ask")
async def ask_question(q: Question):
    result = chain.invoke({"question": q.question})
    return result

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
EOF

# KjÃ¸r API
python server.py
```

---

## Neste steg

1. **KjÃ¸r direkte**: `python -m src.cli ask "Ditt spÃ¸rsmÃ¥l"`
2. **Test embedding API**: Sjekk at `https://fastembed-api.onrender.com/embed` svarer riktig
3. **Finjuster**: Juster prompt-formater eller chain-logikk etter behov

**Alt er rigget for kjÃ¸ring!** ğŸš€
